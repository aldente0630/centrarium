---
layout:     post
title:      Wide & Deep Learning for Recommender Systems
date:       2018-04-28 00:00:00
author:     Google Inc.
categories: Data-Science
---  
  
  
**Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil,Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah의 [*Wide & Deep Learning for Recommender Systems*](https://arxiv.org/pdf/1606.07792v1.pdf)을 번역했습니다.**
  
  
- - -
  
## 초록
  
비선형 변수 변환을 적용한 일반화 선형 모형은 입력값이 희소한 대규모 회귀 분석 및 분류 문제에 널리 사용된다. *광범위한* 교차곱 변수 변환을 통한 변수의 교호 작용 암기는 효과적이고 해석하긴 쉽지만 일반화를 위해서 피쳐 엔지니어링에 더 많은 노력이 필요하다. 적은 피쳐 엔지니어링으로 *심층* 신경망은 희소한 변수에 대해 학습한 저차원 임베딩을 통해 눈에 보이지 않는 변수 조합에 대한 일반화를 보다 잘 할 수 있다. 그러나 임베딩을 통한 심층 신경망은 사용자 - 품목 간 교호 작용이 희소하고 계수가 높을 때 지나치게 일반화되어 크게 관련없는 품목을 추천할 수 있다. 이 논문은 추천 시스템에 암기와 일반화 이점을 결합하기 위해 Wide & Deep 학습, 즉 함께 훈련시킨 광범위한 선형 모형 및 심층 신경망를 제시한다. Google은 10억 명이 넘는 활성 사용자와 100만 개가 넘는 앱을 보유한 상용 모바일 앱 스토어 Google Play에 시스템을 구축하고 평가했다. 온라인 실험 결과에 따르면 Wide & Deep은 광범위한 선형 모형만 사용한 것과 심층 신경망만 사용한 것 대비해서 앱 가입을 크게 증가시켰다. 우린 또한 TensorFlow에 오픈소스 구현해놨다.
  
## 1. 개론
  
추천 시스템은 사용자 및 맥락 정보 집합이 입력 질의문이고 품목마다 순위가 매겨진 목록이 출력인 검색 순위 시스템 일종으로 볼 수 있다. 추천 작업은 질의문이 주어졌을 때 데이터베이스에서 관련 품목을 찾고 클릭 또는 구매 같은 특정 목표에 기반하여 품목 순위를 매기는 것이다.
  
추천 검색 시스템의 한 가지 난점은 일반적인 검색 순위 문제와 마찬가지로 *암기*와 *일반화*를 모두 이뤄내는 것이다. 암기는 동시에 빈발하는 품목 또는 변수를 학습하고 과거 내역에서 이용가능한 상관 관계를 뽑아내 대략적인 정의를 내린다. 한편, 일반화는 상관 관계의 이행성(transtivity)에 기반하고 결코 또는 거의 발생하지 않은 새로운 변수 조합을 탐구한다. 암기에 근거한 추천은 보통 사용자가 이미 행동을 취했던 품목과 직접적으로 관련되어 있다. 암기와 비교할 때, 일반화는 추천 품목 다양성을 향상시키는 경향이 있다. 이 글에서는 Google Play 스토어 앱 추천 문제에 초점을 맞추지만 추천 시스템에 일반적으로 적용해볼 수 있다.
  
기업 내 거대 규모의 온라인 추천 및 순위 시스템에선 로지스틱 회귀 같은 일반화된 선형 모형이 간단하고 확장 가능하며 해석하기 쉽기 때문에 널리 사용된다. 종종 one-hot 인코딩을 사용하여 이진화한 희소 변수에 대해 모형 훈련을 진행한다. 예를 들자면 이진값 변수 "user_installed_app = netflix"는 사용자가 Netflix를 설치한 경우 값 1을 가진다. 암기는 AND(user_installed_app = netflix, impression_app = pandora)와 같은 교차곱 변수 변환을 사용하면 효과적으로 얻어낼 수 있다. 사용자가 Netflix를 설치했고 이후 Pandora가 노출됐으면 값은 1이다. 이는 변수 쌍의 동시 발생이 목표 변수 범주와 어떻게 연관되는지를 설명해준다. AND(user_installed_category = video, impression_category = music) 같이 덜 세분화된 변수를 사용해서 일반화할 수 있지만 수작업 피쳐 엔지니어링이 종종 필요하다. 교차곱 변수 변환의 한 가지 한계는 훈련 데이터에 나타나지 않은 질의문 - 품목 변수 쌍을 일반화하진 못한다는 것이다.
  
분해 기계(factorization machine) 또는 심층 신경망 같은 임베딩 기반 모형은 피쳐 엔지니어링에 대한 부담을 줄이면서 질의문 및 품목 변수마다 저차원의 밀집한 임베딩 벡터를 학습하여 이전에 보지 못한 질의문 - 품목 변수 쌍을 일반화할 수 있다. 그러나 특정 선호도를 가진 사용자나 호소력이 적은 틈새 품목 같이 희소하고 계수가 높은 경우 기본 질의문 - 품목 행렬에 대해 저차원 표현으로 질의문 및 품목을 효과적으로 학습하는건 어렵다. 그런 경우 대부분 질의문 - 품목 쌍들 간에 교호 작용이 없음에도 밀집한 임베딩은 모든 질의문 - 품목 쌍에 대해 0이 아닌 값을 예측할 것이며 따라서 과도하게 일반화되고 별로 관계없는 추천을 할 수 있다. 반면, 교차곱 변수 변환를 통한 선형 모형은 훨씬 적은 수의 매개 변수로 이러한 "예외 규칙"을 암기할 수 있다.
  
본 논문은 그림 1과 같이 선형 모형 구성 요소와 신경망 구성 요소를 함께 학습하여 한 모형 안에서 암기 및 일반화를 모두 달성할 수있는 Wide & Deep 학습 프레임워크를 제시한다.

![Wide & Deep 모형 스펙트럼](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS1.png)
  
본 논문의 주된 기여는 다음과 같다.
  
* 입력값이 희소한 일반 추천 시스템을 위해 임베딩을 통한 피드-포워드 신경망과 변수 변환을 통한 선형 모형을 함께 훈련시키는 Wide & Deep 학습 프레임워크
* 10억 명 이상의 활성 사용자와 100만 개 넘는 앱이 있는 모바일 앱 스토어 인 Google Play에서 제품화된 Wide & Deep 추천 시스템 구현 및 평가
* TensorFlow 고수준 API를 통한 오프소스 구현[^1]
  
아이디어는 단순하지만 Wide & Deep 프레임워크는 모형 훈련 및 제공 속도 요건을 만족시키면서 모바일 앱 스토어 앱 가입율을 크게 향상시켰다.
  
## 2. 추천시스템 개요

앱 추천 시스템에 대한 개요가 그림 2에 나와있다. 사용자가 앱 스토어를 방문하면 사용자 및 맥락 관련된 다양한 변수를 포함하여 질의문이 생성된다. 추천 시스템은 사용자가 클릭이나 구매 같은 특정 동작을 수행할 수 있는 앱 목록(노출이라고도 함)을 반환한다. 사용자 동작은 질의문 및 노출과 함께 학습기를 위한 훈련 데이터로 로그에 기록된다.
  
![추천 시스템 개요](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS2.png)
  
데이터베이스에는 100만 개가 넘는 앱이 있기 때문에 요구되는 서비스 대기 시간(대부분 \\(O(10)\\) 밀리세컨드)이내로 모든 질의문마다 전체 앱에 점수를 철저히 부여하는건 어렵다. 따라서 질의문을 수신한 후 첫번째 단계는 *검색*이다. 검색 시스템은 다양한 신호(일반적으로 기계 학습 모형과 사람이 정의한 규칙의 조합)를 사용하여 질의문과 가장 일치하는 품목의 짧은 목록을 반환한다. 후보 범위를 줄인 후 순위 시스템은 점수로 모든 품목마다 순위를 매긴다. 점수는  대개 \\(P(y\|\mathbf{x})\\), 즉 사용자 변수(예: 국가, 언어, 인구통계학적), 맥락 변수(예: 기기, 시간대, 요일)와 노출 변수(예: 앱 출시 후 경과 기간, 앱 통계 이력)를 포함하여 변수 \\(\mathbf{x}\\)가 주어졌을 때 사용자 동작 각 레이블 \\(y\\)의 확률이다. 본 논문은 Wide & Deep 학습 프레임워크를 사용한 순위 모형에 초점을 맞출 것이다.
  
## 3. Wide & Deep 학습
### 3.1 넓은 부분 구성 요소
  
넓은 부분 구성 요소는 그림 1(좌측)에 그려진 일반화 선형 모형 \\(y=\mathbf{w^Tx}+b\\)의 형태이다. \\(y\\)는 예측, \\(\mathbf{x}=[x_1, x_2, ..., x_d]\\)는 \\(d\\)개의 변수 벡터, \\(\mathbf{w}=[w_1, w_2, ..., w_d]\\)는 모형 매개변수이고 \\(b\\)는 편의이다. 변수 집합은 원천 입력값들과 변환한 변수들을 포함한다. 가장 중요한 변환 중 하나는 다음과 같이 정의되는 *교차곱 변수 변환*이다.
  
$$\phi_k(\mathbf{x})=\prod_{i=1}^d x^{c_{ki}}_i, c_{ki}\in\{0,1\}$$
  
여기서 \\(c_{ki}\\)는 \\(i\\)번째 변수가 \\(k\\)번째 변환 \\(\phi_k\\)의 일부이면 1이고 그렇지 않으면 0인 불리언 변수이다. 이진값 변수의 경우 구성 변수들("gender=female"과 "language=en")이 모두 1인 경우에만 교차곱 변수 변환(예: "AND(gender = female, language = en)")은 1이다. 그렇지 않으면 0이다. 이것은 이진값 변수 간 교호 작용을 잡아내고 일반화 선형 모형에 비선형성을 추가한다.
  
### 3.2 깊은 부분 구성 요소
  
깊은 부분 구성 요소는 그림 1(우측)과 같은 피드-포워드 신경망이다. 범주형 변수의 경우 원래 입력값은 특성 문자열(예: "language = en")이다. 이런 희소하고 고차원인 범주형 변수 각각은 먼저 임베딩 벡터라고 하는 저차원 밀집한 실수값 벡터로 종종 변환된다. 임베딩 차원은 일반적으로 \\(O(10)\\)에서 \\(O(100)\\) 수준으로 정한다. 임베딩 벡터는 임의로 초기화된 후 모형 훈련 과정을 통해 최종 손실 함수를 최소화하도록 값이 훈련된다. 이러한 저차원의 밀집한 임베딩 벡터는 순방향 전달 때 신경망의 은닉층으로 전해진다. 구체적으로 각 은닉층은 다음 계산을 수행한다.
  
$$a^{(l+1)}=f(W^{(l)}a^{(l)}+b^{(l)})$$
  
여기서 \\(l\\)은 층의 서수이고 \\(f\\)는 종종 정류된 선형 단위(ReLU)라고 불리는 활성화 함수이다. \\(a^{(l)}\\), \\(b^{(l)}\\)와 \\(W^{(l)}\\)는 \\(l\\)번째 층의 출력값, 편의, 모형 가중치이다.
  
### 3.3 Wide & Deep 모형의 공동 훈련
  
Wide 구성 요소와 Deep 구성 요소는 결합되어 출력한 로그 오즈 가중치 합계를 예측치로 사용하며 공동 훈련을 위한 하나의 공통 로지스틱 손실 함수에 제공된다. *공동 훈련*과 *앙상블* 사이에 구별이 있다는 걸 유의해라. 앙상블에서 개별 모형은 서로 알지 못하게 각자 훈련되며 그것들의 예측치는 추론 시기에만 결합되고 훈련 시기에는 결합되지 않는다. 대조적으로 공동 훈련은 넓은 부분과 깊은 부분 모두와 그것들의 합계 가중치를 훈련 시기에 고려하면서 모든 매개변수를 동시에 최적화한다. 모형 크기에도 영향을 미친다. 앙상블의 경우 훈련이 분리되어 있으므로 합리적인 정확도를 얻기 위해 모형 각각이 좀 더 커야한다(예: 더 많은 변수와 변수 변환). 이와는 다르게 공동 훈련의 경우 넓은 부분은 전체 크기의 Wide 모형보다 적은 수의 교차곱 변수 변환으로 깊은 부분의 약점을 보완하기만 하면 된다.
  
Wide & Deep 모형 공동 훈련은 미니 배치 확률적 최적화를 이용하여 출력값 기울기를 모형 넓은 부분과 깊은 부분 동시에 역전파시켰다. 실험에서 모형 넓은 부분에 대한 최적화로 L1 정규화를 따르는 Follow-the-regularized-leader(FTRL) 알고리즘을 사용했고 깊은 부분에 대해서는 AdaGrad를 사용했다.
  
결합된 모형은 그림1(중앙)에 나와있다. 로지스틱 회귀 문제의 경우 모형 예측은 다음과 같다.
  
$$P(Y=1|\mathbf{x})=\sigma(\mathbf{w^T_{wide}}[\mathbf{x},\phi(\mathbf{x})]+\mathbf{w^T_{deep}}a^{(l_f)}+b)$$
  
여기서 \\(Y\\)는 이진값 클래스 레이블이고 \\(\sigma(\cdot)\\)는 시그모이드 함수, \\(\phi(\mathbf{x})\\)는 원래 변수 \\(\mathbf{x}\\)의 교차곱 변수 변환, \\(b\\)는 편의항이다. \\(\mathbf{w_{wide}}\\)는 Wide 모형의 모든 가중치 벡터이고 \\(\mathbf{w_{deep}}\\)은 최종 출력값 \\(a^{(l_f)}\\)에 적용된 가중치이다.
  
## 4. 시스템 구현
  
앱 추천 파이프라인 구현은 데이터 생성, 모형 훈련 및 모형 제공 같은 3단계로 구성된다(그림 3 참조).
  
![앱 추천 파이프라인 개요](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS3.png)
  
### 4.1 데이터 생성
  
이 단계에서는 일정 기간 내 사용자 및 앱 노출 데이터를 사용하여 훈련 데이터를 생성한다. 각 샘플은 노출 한 번에 해당한다. 레이블은 *앱 가입*이다. 즉, 노출한 앱을 설치하면 1이고 그렇지 않으면 0이다. 
  
범주형 변수 문자열을 정수 ID로 연결하는 어휘 테이블도 이 단계에서 생성된다. 시스템은 최소 횟수 이상으로 발생하는 문자열 변수 모두에 대해 ID 공간을 계산한다. 연속적인 실수값 변수는 변수값 \\(x\\)를 누적 분포 함수 \\(P(X\le x)\\)에 연결하여 \\(n_q\\) 분위수로 나누어 \\([0, 1]\\)로 정규화한다. 정규화한 값은 \\(i\\)번째 분위수 값에 대해 \\(\frac{i-1}{n_q-1}\\)이다. 데이터 생성 동안 분위값 경계가 계산된다.

### 4.2 모형 훈련
  
실험에서 사용한 모형 구조는 그림 4와 같다. 훈련 과정에서 입력층은 훈련 데이터와 어휘를 받아들여 레이블과 함께 희소하거나 밀집된 변수를 생성한다. Wide 구성 요소는 사용자가 설치 한 앱과 노출 앱의 교차 제품 변환으로 구성됩니다. 모델의 깊은 부분에 대해 32 차원 임베딩 벡터가 각 범주 형 기능에 대해 학습됩니다. 모든 임베딩을 고밀도 피쳐와 연결하여 약 1200 차원의 고밀도 벡터를 생성합니다. 연결된 벡터는 3 ReLU 레이어로 전달되고 마지막으로 물류 출력 단위로 전달됩니다.

![앱 추천을 위한 Wide & Deep 모형 구조](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS4.png)
  
Wide & Deep 모델은 5 천억 개가 넘는 예제를 교육합니다. 새로운 일련의 학습 데이터가 도착할 때마다,
모델을 다시 훈련해야합니다. 그러나 매번 처음부터 재교육하는 것은 계산 비용이 많이 들고 데이터가 도착한 후 업데이트 된 모델을 제공하는 데 걸리는 시간을 지연시킵니다. 이 과제를 해결하기 위해 우리는 임베딩과 이전 모델의 선형 모델 가중치를 사용하여 새 모델을 초기화하는 웜 스타트 시스템을 구현했습니다. 
  
모델 서버에 모델을로드하기 전에 실제 트래픽을 처리 할 때 문제가 발생하지 않도록 모델을 실행합니다. 이전 모델에 대한 모델 품질을 온 전성 검사로 경험적으로 검증합니다.

### 4.2 모형 제공
  
모델이 훈련되고 검증되면 모델 서버에 모델을로드합니다. 각 요청에 대해 서버는 앱 검색 시스템에서 앱 후보 집합을 수신하고 사용자 기능을 사용하여 각 앱에 점수를 매 깁니다. 그런 다음 앱은 가장 높은 점수에서 가장 낮은 점수로 순위가 매겨지며이 순서대로 사용자에게 앱을 표시합니다. 점수는 Wide & Deep 모델에 대해 순방향 추론을 실행하여 계산됩니다.
  
10 밀리 초 단위로 각 요청을 처리하기 위해 단일 배치 추론 단계에서 모든 후보 앱을 채점하는 대신 작은 배치를 병렬로 실행하여 멀티 스레딩 병렬 처리를 사용하여 성능을 최적화했습니다.

## 5. 실험 결과
  
실제 추천 시스템에서 Wide & Deep 학습의 효과를 평가하기 위해 실제 실험을 실행하고 앱 가입 및 서비스 제공이라는 두 가지 측면에서 시스템을 평가했습니다.

### 5.1 앱 가입

[^1]: [http://tensorflow.org](https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/wide_and_deep)의 Wide & Deep 튜토리얼을 보라.
