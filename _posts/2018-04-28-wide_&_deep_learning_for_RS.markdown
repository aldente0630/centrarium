---
layout:     post
title:      Wide & Deep Learning for Recommender Systems
date:       2018-04-28 00:00:00
author:     Google Inc.
categories: Data-Science
---  
  
  
**Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil,Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah의 [*Wide & Deep Learning for Recommender Systems*](https://arxiv.org/pdf/1606.07792v1.pdf)을 번역했습니다.**
  
  
- - -
  
## 초록
  
입력값이 희소한 대규모 회귀 분석과 분류 문제에 비선형 변수 변환을 적용한 일반화 선형 모형을 흔히 사용한다. *광범위한* 교차곱 변수 변환을 통한 변수 교호 작용 암기는 효과적이고 해석하긴 쉽지만 일반화를 위해서 피쳐 엔지니어링에 더 많은 노력이 필요하다. *깊은* 신경망은 희소한 변수에 대해 학습한 저차원 임베딩을 통해 적은 피쳐 엔지니어링으로 눈에 보이지 않는 변수 조합에 대한 일반화를 보다 더 잘 할 수 있다. 그러나 임베딩을 통한  신경망은 사용자 - 품목 간 교호 작용이 희소하고 계수가 높은 경우 지나치게 일반화하기 때문에 크게 관련없는 품목을 추천할 수 있다. 이 논문은 암기와 일반화 이점을 결합하기 위해 추천 시스템에 Wide & Deep 학습, 즉 넓은 선형 모형과 깊은 신경망을 함께 훈련시킨 방법론을 제시한다. 우리는 10억 명이 넘는 활성 사용자와 100만 개가 넘는 앱을 보유한 상용 모바일 앱 스토어 Google Play에 시스템을 구축하고 평가했다. 온라인 실험 결과에 따르면 Wide & Deep은 넓은 선형 모형만 사용한 것과 깊은 신경망만 사용한 것 대비해서 앱 가입을 크게 증가시켰다. 덧붙여 TensorFlow에 오픈소스로 구현해놨다.
  
## 1. 개론
  
추천 시스템은 사용자 및 맥락 정보 집합이 입력 질의문이고 품목마다 순위가 매겨진 목록이 출력인 검색 순위 시스템 일종으로 볼 수 있다. 추천 작업은 질의문이 주어졌을 때 데이터베이스에서 관련 품목을 찾고 클릭 또는 구매 같은 특정 목표에 기반하여 품목 순위를 매기는 것이다.
  
추천 검색 시스템의 한 가지 난점은 일반적인 검색 순위 문제와 마찬가지로 *암기*와 *일반화*를 모두 달성하는 것이다. 암기는 동시에 빈발하는 품목 또는 변수를 학습하고 과거 이력에서 이용가능한 상관 관계를 뽑아내는 작업으로 대략 정의된다. 한편, 일반화는 상관 관계의 이행성(transtivity)에 기반하고 결코 또는 거의 발생하지 않은 새로운 변수 조합을 탐구한다. 암기에 근거한 추천은 보통 사용자가 이미 행동을 취했던 품목과 직접적으로 관련되어 있다. 암기와 비교할 때 일반화는 추천 품목 다양성을 보통 향상시킨다. 이 글에서는 Google Play 스토어 앱 추천 문제를 다루지만 일반적인 추천 시스템에 적용해볼 수 있다.
  
기업 내 대형 온라인 추천 및 순위 시스템은 로지스틱 회귀 같은 일반화된 선형 모형이 간단하고 확장 가능하며 해석하기 쉽기 때문에 널리 사용한다. 종종 one-hot 인코딩을 사용하여 이진화한 희소 변수에 대해 모형 훈련을 진행한다. 예를 들자면 이진값 변수 "user_installed_app = netflix"는 사용자가 Netflix를 설치한 경우 값 1을 가진다. 암기는 AND(user_installed_app = netflix, impression_app = pandora)와 같이 교차곱 변수 변환을 사용하면 효과적으로 만들 수 있다. 사용자가 Netflix를 설치했고 이후 Pandora에 노출됐으면 값은 1이다. 이는 변수 쌍 동시 발생이 목표 변수 레이블과 어떻게 연관되는지를 설명해준다. AND(user_installed_category = video, impression_category = music) 같이 세분화가 덜 된 변수를 사용해서 일반화할 수 있지만 피쳐 엔지니어링 수작업이 많이 필요하다. 교차곱 변수 변환 한계점 중 하나는 훈련 데이터에 나타나지 않은 질의문 - 품목 변수 쌍을 일반화하진 못한다는 점이다.
  
분해 기계(factorization machine) 또는  신경망 같은 임베딩 기반 모형은 피쳐 엔지니어링에 대한 부담을 줄이면서 질의문 및 품목 변수마다 저차원의 밀집 임베딩 벡터를 학습하여 이전에 보지 못한 질의문 - 품목 변수 쌍을 일반화할 수 있다. 그러나 특정 선호도를 가진 사용자나 크게 어필 하지 못하는 틈새 품목 같이 희소하고 계수가 높은 경우 기본 질의문 - 품목 행렬에 대해 저차원 표현으로 질의문과 품목을 효과적으로 학습하는건 어렵다. 그런 경우 대부분의 질의문 - 품목 쌍 간에 교호 작용이 없음에도 밀집 임베딩은 모든 질의문 - 품목 쌍에 대해 0이 아닌 값을 예측할 것이고 따라서 과도하게 일반화하고 별로 관계없는 추천을 할 수 있다. 반면 교차곱 변수 변환를 통한 선형 모형은 훨씬 적은 수의 매개 변수로 이러한 "예외 규칙"을 암기할 수 있다.
  
본 논문은 그림 1과 같이 선형 모형 구성 요소와 신경망 구성 요소를 함께 학습하여 한 모형 안에서 암기 및 일반화 모두를 달성할 수있는 Wide & Deep 학습 프레임워크를 제시한다.

![Wide & Deep 모형 스펙트럼](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS1.png)
  
본 논문의 주된 기여는 다음과 같다.
  
* 입력값이 희소한 일반 추천 시스템을 위해 임베딩을 통한 피드-포워드 신경망과 변수 변환을 통한 선형 모형을 함께 훈련시키는 Wide & Deep 학습 프레임워크
* 10억 명 이상의 활성 사용자와 100만 개 넘는 앱이 있는 모바일 앱 스토어 인 Google Play에서 Wide & Deep 추천 시스템 제품화 구현 및 평가
* TensorFlow 고수준 API를 통한 오프소스 구현[^1]
  
아이디어는 단순하지만 Wide & Deep 프레임워크는 모형 훈련 및 서비스 속도 요건을 만족시키면서 모바일 앱 스토어 앱 가입율을 크게 향상시켰다.
  
## 2. 추천시스템 개요

앱 추천 시스템에 대한 개요가 그림 2에 나와있다. 사용자가 앱 스토어를 방문하면 사용자 본인과 맥락에 관련된 다양한 변수가 포함되어 질의문이 생성된다. 추천 시스템은 사용자가 클릭이나 구매 같은 특정 동작을 수행할 수 있는 앱 목록(노출이라고도 함)을 반환한다. 사용자 동작은 질의문 및 노출과 함께 학습기를 위한 훈련 데이터로 로그에 기록된다.
  
![추천 시스템 개요](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS2.png)
  
데이터베이스에는 100만 개가 넘는 앱이 있기에 요구되는 서비스 대기 시간(대부분 \\(O(10)\\) 밀리세컨드)이내로 모든 질의문마다 전체 앱에 점수를 철저히 매기는건 어렵다. 따라서 질의문 수신 후 첫번째 단계는 *검색*이 된다. 검색 시스템은 다양한 신호(일반적으로 기계 학습 모형과 사람이 정의한 규칙 조합)를 사용하여 질의문과 가장 일치하는 품목의 짧은 목록을 반환한다. 후보 범위를 줄인 후 순위 시스템은 모든 품목마다 점수 순위를 매긴다. 점수는  대개 \\(P(y\|\mathbf{x})\\), 즉 사용자 변수(예: 국가, 언어, 인구통계학적), 맥락 변수(예: 기기, 시간대, 요일)와 노출 변수(예: 앱 출시 후 경과 기간, 앱 통계 이력)를 포함하여 변수 \\(\mathbf{x}\\)가 주어졌을 때 사용자 동작 각 레이블 \\(y\\)의 확률이다. 본 논문은 Wide & Deep 학습 프레임워크를 사용한 순위 모형에 초점을 맞출 것이다.
  
## 3. Wide & Deep 학습
### 3.1 넓은 쪽 구성 요소
  
넓은 쪽 구성 요소는 그림 1(좌측)에 그려진 일반화 선형 모형 \\(y=\mathbf{w^Tx}+b\\)의 형태이다. \\(y\\)는 예측, \\(\mathbf{x}=[x_1, x_2, ..., x_d]\\)는 \\(d\\)개의 변수 벡터, \\(\mathbf{w}=[w_1, w_2, ..., w_d]\\)는 모형 매개변수이고 \\(b\\)는 편의이다. 변수 집합은 원천 입력값과 변환한 변수를 포함한다. 가장 중요한 변환 중 하나는 다음과 같이 정의되는 *교차곱 변수 변환*이다.
  
$$\phi_k(\mathbf{x})=\prod_{i=1}^d x^{c_{ki}}_i, c_{ki}\in\{0,1\}$$
  
여기서 \\(c_{ki}\\)는 \\(i\\)번째 변수가 \\(k\\)번째 변환 \\(\phi_k\\)의 일부이면 1이고 그렇지 않으면 0인 불리언 변수이다. 이진값 변수의 경우 구성 변수들("gender=female"과 "language=en")이 모두 1인 경우에만 교차곱 변수 변환(예: "AND(gender = female, language = en)")은 1이다. 그렇지 않으면 0이다. 이것은 이진값 변수 간 교호 작용을 잡아내고 일반화 선형 모형에 비선형성을 더한다.
  
### 3.2 깊은 쪽 구성 요소
  
깊은 쪽 구성 요소는 그림 1(우측)과 같은 피드-포워드 신경망이다. 범주형 변수의 경우 원래 입력값은 특성 문자열(예: "language = en")이다. 이런 희소하고 고차원인 범주형 변수 각각은 먼저 임베딩 벡터라고 하는 저차원의 밀집한 실수값 벡터로 종종 변환된다. 임베딩 차원은 일반적으로 \\(O(10)\\)에서 \\(O(100)\\) 수준으로 정한다. 임베딩 벡터는 임의로 초기화된 후 모형 훈련 과정을 통해 최종 손실 함수를 최소화하도록 값이 훈련된다. 이러한 저차원의 밀집한 임베딩 벡터는 순방향 전달 때 신경망 은닉층으로 전해진다. 구체적으로 각 은닉층은 다음 계산을 수행한다.
  
$$a^{(l+1)}=f(W^{(l)}a^{(l)}+b^{(l)})$$
  
여기서 \\(l\\)은 층의 서수이고 \\(f\\)는 종종 정류된 선형 단위(ReLU)라고 불리는 활성화 함수이다. \\(a^{(l)}\\), \\(b^{(l)}\\)와 \\(W^{(l)}\\)는 \\(l\\)번째 층의 출력값, 편의, 모형 가중치이다.
  
### 3.3 Wide & Deep 모형의 공동 훈련
  
넓은 쪽 구성 요소와 깊은 쪽 구성 요소는 결합되어 출력한 로그 오즈 가중치 합계를 예측치로 사용하며 공동 훈련을 위해 단일한 공통 로지스틱 손실 함수에 제공된다. *공동 훈련*과 *앙상블* 사이에 구별이 있다는 걸 유의해라. 앙상블에서는 개별 모형을 서로 알지 못하게 각기 훈련하며 그 예측치들은 추론 시기에만 결합하고 훈련 시기에는 결합하지 않는다. 대조적으로 공동 훈련은 넓은 쪽과 깊은 쪽 모두와 그 합계 가중치들을 훈련 시기에 고려하면서 매개변수 모두를 동시에 최적화시킨다. 모형 크기에도 영향을 미친다. 앙상블의 경우 훈련이 분리되어 있으므로 합리적인 정확도를 얻기 위해 모형 각각이 좀 더 커야한다(예: 더 많은 변수와 변수 변환). 이와는 다르게 공동 훈련의 경우 넓은 쪽은 전체 크기의 Wide 모형보다 적은 수의 교차곱 변수 변환으로 깊은 쪽 약점을 보완하기만 하면 된다.
  
Wide & Deep 모형 공동 훈련은 미니 배치 확률적 최적화를 이용하여 출력값 기울기를 모형 넓은 쪽과 깊은 쪽 동시에 역전파시킨다. 실험에서 모형 넓은 쪽에 대한 최적화로 L1 정규화를 따르는 Follow-the-regularized-leader(FTRL) 알고리즘을 사용했고 깊은 쪽에 대해서는 AdaGrad를 사용했다.
  
결합 모형은 그림1(중앙)에 나와있다. 로지스틱 회귀 문제의 경우 모형 예측은 다음과 같다.
  
$$P(Y=1|\mathbf{x})=\sigma(\mathbf{w^T_{wide}}[\mathbf{x},\phi(\mathbf{x})]+\mathbf{w^T_{deep}}a^{(l_f)}+b)$$
  
여기서 \\(Y\\)는 이진값 클래스 레이블이고 \\(\sigma(\cdot)\\)는 시그모이드 함수, \\(\phi(\mathbf{x})\\)는 원래 변수 \\(\mathbf{x}\\)의 교차곱 변수 변환, \\(b\\)는 편의 항이다. \\(\mathbf{w_{wide}}\\)는 Wide 모형의 모든 가중치 벡터이고 \\(\mathbf{w_{deep}}\\)은 최종 출력값 \\(a^{(l_f)}\\)에 적용한 가중치이다.
  
## 4. 시스템 구현
  
앱 추천 파이프라인 구현은 데이터 생성, 모형 훈련 및 모형 서비스 같은 3단계로 구성된다(그림 3 참조).
  
![앱 추천 파이프라인 개요](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS3.png)
  
### 4.1 데이터 생성
  
이 단계에서는 일정 기간 내 사용자 및 앱 노출 데이터를 사용하여 훈련 데이터를 생성한다. 각 샘플은 노출 한 번에 해당한다. 레이블은 *앱 가입*이다. 즉, 노출한 앱을 설치하면 1이고 그렇지 않으면 0이다. 
  
범주형 변수 문자열을 정수 ID로 연결하는 어휘 테이블도 이 단계에서 생성한다. 시스템은 정한 최소 횟수 이상으로 발생하는 모든 문자열 변수에 대해 ID 공간을 계산한다. 연속적인 실수값 변수는 변수값 \\(x\\)를 누적 분포 함수 \\(P(X\le x)\\)에 연결하여 \\(n_q\\) 분위수로 나누어 \\([0, 1]\\)로 정규화한다. 정규화한 값은 \\(i\\)번째 분위수 값에 대해 \\(\frac{i-1}{n_q-1}\\)이다. 데이터 생성 동안 분위값 경계를 계산한다.

### 4.2 모형 훈련
  
실험에서 사용한 모형 구조는 그림 4와 같다. 훈련 과정에서 입력층은 훈련 데이터와 어휘를 받아들여 레이블과 함께 희소하거나 밀집한 변수를 생성한다. 넓은 쪽 구성 요소는 사용자가 설치한 앱과 노출된 앱의 교차곱 변수 변환으로 구성한다. 모형 깊은 쪽은 32차원 임베딩 벡터가 각 범주형 변수에 대해 학습한다. 모든 임베딩를 밀집 변수와 연결하여 약 1,200 차원 밀집 벡터를 생성한다. 연결한 벡터를 3개의 ReLU 층으로 전달하고 마지막으로 로지스틱 출력 단위로 전달한다.

![앱 추천을 위한 Wide & Deep 모형 구조](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS4.png)
  
Wide & Deep 모형을 5천억 개가 넘는 샘플로 훈련시킨다. 일련의 새로운 훈련 데이터가 수집될 때마다 모형을 다시 훈련시켜야한다. 그러나 매번 처음부터 재훈련시키는건 계산 비용이 많이 들고 데이터 수집부터 업데이트된 모형 서비스까지 시간이 많이 소요된다. 이 문제를 해결하기 위해 임베딩과 이전 모형의 선형 모형 가중치를 사용하여 새 모형 초기값을 설정하는 식의 웜 스타트 시스템을 구현했다. 
  
모형 서버에 모형을 적재하기 전에 실제 트래픽을 처리하는데 문제가 없는지 예행 연습 삼아 모형을 돌려보았다. 새너티 테스트로써 이전 모형 대비 모형 품질을 직접 확인해보았다.

### 4.2 모형 서비스
  
모형을 훈련하고 검증이 끝나면 모형 서버에 모형을 적재한다. 각 요청마다 서버는 앱 검색 시스템에서 앱 후보군을 수신하고 사용자 변수를 사용하여 모든 앱에 점수를 매긴다. 앱은 가장 높은 점수부터 가장 낮은 점수까지 순위를 매기며 그 순서로 사용자에게 노출한다. 점수는 Wide & Deep 모형에 대한 순방향 추론을 시행하여 계산한다.
  
10ms 단위로 각 요청을 처리하기 위해 추론 단계를 단일 배치로 후보 앱 전체에 점수를 매기는 대신 멀티스레딩 병렬 처리를 통해 미니 배치를 병렬로 돌려서 성능을 최적화했다.

## 5. 실험 결과
  
현실 상의 추천 시스템에서 Wide & Deep 학습 효과를 평가하기 위해 실제 실험을 진행했고 앱 가입 및 서비스 성능 두 가지 측면에서 시스템을 평가했다.

### 5.1 앱 가입
  
3주 동안 A/B 테스트 프레임워크로 온라인 실험을 실제 환경에서 진행했다. 대조군으로 사용자 1%를 무작위로 선정해서 이전 버전의 순위 모형 즉 교차곱 변수 변환을 다수 집어넣어 높은 수준으로 최적화시킨, 넓은 쪽만 이용한 로지스틱 회귀 모형을 적용해 추천했다. 실험군인 1%의 다른 사용자에게는 동일한 변수 집합으로 훈련시킨 Wide & Deep 모형을 적용하여 추천했다. 표 1에서와 같이 Wide & Deep 모형은 앱 스토어 방문 페이지 상의 앱 가입율을 대조군 대비 3.9% 향상시켰다(통계적으로 유의미함). 또 다른 1% 그룹에 변수는 동일하고 신경망 구조의 깊은 쪽만 사용한 모형을 적용하여 결과를 비교했고 Wide & Deep 모형은 깊은 쪽만 사용한 모형 대비해서 가입율이 1% 증가시켰다(통계적으로 유의미함).
  
![여러 모형에 대한 오프라인과 온라인 측도. 온라인 가입율 증가는 대조군 대비이다.](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS5.png)
  
온라인 실험에 덧붙여 오프라인에서 미리 덜어놓은 데이터셋에 대해 수신자 조작 특성 곡선 아래 면적(AUC)을 측정했다. 오프라인 AUC 또한 Wide & Deep 모형이 다소 높지만 온라인 트래픽에서 영향력이 더 세다. 가능한 이유 중 하나로 오프라인 데이터셋 노출과 레이블은 고정되어있지만 온라인 시스템은 일반화와 암기를 혼합하여 탐색적으로 추천해볼 수 있고 그에 따른 사용자 응답으로 학습이 가능하다는 점이다.

### 5.2 서비스 성능

상용 모바일 앱 스토어는 트래픽이 많기 때문에 짧은 대기 시간 내 처리량을 높게 유지하면서 서비스하기가 까다롭다. 트래픽 최고 수준에서 추천 서버는 1 초 당 천만 개 이상 앱에 점수를 매긴다. 단일 스레딩을 사용하는 경우 모든 후보를 일괄 처리로 점수 매기는데 31ms가 걸린다. 멀티스레딩을 구현하고 각 배치 크기를 작게 분할하여 표 2에 표시된대로 클라이언트 측 대기 시간을 14ms(서비스 오버헤드 포함)로 크게 줄였다.
  
![서비스 대기 시간 vs 배치 사이즈와 스레드 수](https://aldente0630.github.io/assets/wide_&_deep_learning_for_RS6.png)
  
## 6. 관련 작업
  
교차곱 변수 변환을 사용한 넓은 선형 모형과 밀집 임베딩을 사용한 깊은 신경망을 결합하는 아이디어는 두 변수 간 교호작용을 2개의 저차원 임베딩 벡터 간 내적으로 분해해서 선형 모형에 일반화를 더하는 분해 기계 같이 이전 작업에서 시작됐다. 이 논문은 내적 대신 신경망을 통한 임베딩 간 고차 비선형 교호작용을 학습해서 모형 수용 능력을 높였다.

언어 모형에서 입출력 간 직접적인 가중치를 학습함으로써 RNN 복잡도(예: 은닉층 크기)를 크게 줄이는 형태의 순환신경망(RNNs)과 n-gram 변수를 사용한 최대 엔트로피 모형을 공동 훈련시키는 방법론이 제안되었다. 컴퓨터 비전에서는 깊은 잔여 학습을 사용해 모형을 더 깊게 훈련시킬 때의 어려움을 줄였고 하나 이상의 층을 건너뛰는 지름길 연결을 통해 정확도를 향상시켰다. 이미지로부터 사람 자세를 추정하는 문제에도 그래픽 모형을 사용한 신경망 공동 훈련을 적용했다. 이 작업은 일반적인 추천과 순위 지정 문제에 대해 희소 변수와 출력 단위 사이를 직접 연결해서 피드-포워드 신경망과 선형 모형을 공동 훈련시키는 방법을 탐구했다.

추천 시스템 논문들은 등급 행렬에 대한 컨텐츠 정보와 협업 필터링(CF)을 결합한 딥러닝을 사용해 협업 딥러닝을 탐구하였다. 또한 AppJoy 같이 사용자 앱 사용 기록을 CF에 사용하는 모바일 앱 추천 시스템도 있었다. CF 기반 또는 콘텐츠 기반 접근 방식의 이전 작업과는 달리 우리는 앱 추천 시스템의 사용자와 노출 데이터에 대해 Wide & Deep 모형을 공동 훈련시켰다.
  
## 7. 결론
  
추천 시스템에서 암기와 일반화는 중요하다. 넓은 선형 모형은 교차곱 변수 변환을 통해 희소한 변수 간 교호 작용을 효과적으로 암기할 수 있지만 깊은 신경망은 저차원 임베딩을 통해 이전에 보이지않던 변수 간 교호 작용을 일반화할 수 있다. 두 가지 모형 유형의 장점을 결합하기 위해 Wide & Deep 학습 프레임워크를 제시했다. 대규모 상업 앱 스토어 Google Play 추천 시스템에서 프레임워크를 구축하고 평가했다. 온라인 실험 결과에 따르면 Wide & Deep 모형은 넓은 쪽만 사용한 모형과 깊은 쪽만 사용한 모형 대비해서 앱 가입율을 크게 향상시켰다.
  
[^1]: ![CDATA[http://tensorflow.org]](https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/wide_and_deep)의 Wide & Deep 튜토리얼을 보라.
