---
layout: post
title: LightGBM 고효율 그래디언트 부스팅 결정 트리
date: 2018-06-29 00:00:00
author: Microsoft Research, Peking University, Microsoft Redmond
categories: Data-Science
---  
  
  
**Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu의 [*LightGBM: A Highly Efficient Gradient Boosting Decision Tree*](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)을 번역했습니다.**
  
  
- - -
  
## 초록
그래디언트 부스팅 결정 트리(GBDT)는 널리 사용되는 기계 학습 알고리즘이며 XGBoost와 pGBRT 같이 효율적으로 구현해놓은 것이 몇 가지 있다. 해당 구현은 엔지니어링의 많은 요소를 최적화시켰지만 고차원 변수에 데이터 크기가 큰 경우 효율성과 확장성은 여전히 불만족스럽다. 주된 이유로 각 변수마다 가능한 모든 분할점에 대해 정보 획득을 평가하려면 데이터 개체 모두 훑어야하는데 이에 많은 시간이 소요된다는 점이다. 이 문제를 해결하기 위해 *기울기 기반 단측 표본추출*(GOSS)과 *배타적 변수 묶음*(EFB)이라는 새로운 기술 두 가지를 제안한다. GOSS를 통해 데이터 개체 중 기울기가 작은 상당 부분을 제외시키고 나머지만 사용하여 정보를 얻을 수 있다. 기울기가 큰 데이터 개체가 정보 획득 계산에 더 중요한 역할을 하기에 GOSS는 훨씬 작은 크기의 데이터로 정보 획득을 매우 정확하게 추정할 수 있다. EFB를 통해 변수 개수를 줄이기 위해 상호 배타적 변수들(예컨대, 0이 아닌 값을 동시에 갖는 일이 거의 없는 변수들)을 묶는다. 배타적 변수의 최적 묶음을 찾는 일은 NP-hard지만 탐욕 알고리즘을 통해 매우 좋은 근사 비율을 얻을 수 있다. 따라서 분할점 결정 정확도를 크게 훼손시키지 않으면서 변수 개수를 효과적으로 줄일 수 있다. GOSS와 EFB 적용하여 GBDT를 새롭게 구현한 걸 *LightGBM*라고 부르겠다. 여러 공용 데이터셋에 대한 실험은 LightGBM이 기존 GBDT 훈련 과정을 최대 20배 이상 빠르게 하면서 정확도는 거의 동일하게 달성함을 보여준다.
  
## 1. 개론
그래디언트 부스팅 결정 트리(GBDT)는 효율성, 정확도, 해석 가능성이 높아 널리 사용되는 기계 학습 알고리즘이다. GBDT는 멀티 클래스 분류, 클릭 예측, 순위 학습 같이 다양한 기계 학습 작업에서 최고의 성능을 보여준다. 최근 빅데이터(변수 개수와 개체 수 모두) 등장으로 GBDT는 특히 정확도와 효율성 간의 트레이드오프라는 새로운 숙제에 직면했다. GBDT 기존 구현은 각 변수마다 가능한 모든 분할점에 대해 정보 획득을 평가하기 위해 데이터 개체 모두 훑어야한다. 따라서 계산 복잡도는 변수 개수와 개체 수에 비례한다. 따라서 빅데이터 처리 시 해당 구현은 매우 시간 소모적이게 된다.
  
데이터 개체 수와 변수 개수를 줄이는게 문제 해결을 위한 즉각적 아이디어다. 그러나 이 작업은 꽤나 단순하지 않다. 예를 들어 GBDT를 위해 데이터 표본 추출 수행하는 방법이 불명확하다. 부스팅 훈련 과정의 속도를 향상시키기 위해 가중치에 따라 데이터 표본 추출하는 작업물이 있지만 GBDT에는 표본 가중치가 전혀 없으므로 직접 적용할 수 없다. 본 논문은 목표를 위해 두 가지 새로운 기술을 제안한다.
  
*기울기 기반 단측 표본추출*(GOSS). GBDT의 경우 데이터 개체에 대한 기본 가중치는 없지만 서로 다른 기울기를 가진 데이터 개체가 정보 획득 계산 시 서로 다른 역할을 한다는 점은 알고있다. 정보 획득의 정의로 보자면 기울기가 보다 큰[^1](즉, 과소 훈련시킨 개체)개체가 정보 획득에 더 기여할 것이다. 따라서 데이터 개체를 다운샘플링할 때 정보 획득 추정의 정확도를 유지하려면 기울기가 큰 개체(예컨대 미리 정의한 임계 값보다 크거나 백분위로 상위인)를 보다 많이 유지해야하며 기울기가 작은 개체는 무작위로 떨궈야한다. 이런 방식이 타겟 변수마다 동일한 표본 추출 비율로 균일하게 무작위 표본 추출하는 것보다 더 정확하게 정보 획득을 추정할 수 있음(정보 획득 값의 범위가 큰 경우 특히나)을 증명해낼 것이다. 
  
*배타적 변수 묶음*(EFB). 실제 응용 프로그램은 일반적으로 많은 수의 변수를 갖지만 변수 공간은 매우 희소하여 변수 개수를 효과적으로 줄이기 위한 거의 손실 없는 방식을 만들어낼 수 있다. 특히 희소한 변수 공간에서 많은 변수들이 (거의) 배타적이다. 즉, 변수들은 0이 아닌 값을 동시에 갖는 일이 거의 없다 . 예로 원-핫 변수(텍스트 마이닝의 원-핫 단어 표현 같은)가 있다. 이런 배타적 변수들은 안전하게 묶을 수 있다. 결국 최적 묶음 문제를 그래프 채색 문제(변수를 각 꼭지점에 두고 두 변수가 상호 배타적이지 않으면 두 변수를 잇는 선을 추가함으로써)로 바꾸는 효율적인 알고리즘을 설계했고 일정 근사 비율을 갖는 탐욕 알고리즘으로 문제를 해결했다.
  
GOSS와 EFB 적용하여 GBDT를 새롭게 구현한 걸 *LightGBM*[^2]라고 부르겠다. 여러 공용 데이터셋에 대한 실험은 LightGBM이 기존 GBDT 훈련 과정을 최대 20배 이상 빠르게 하면서 정확도는 거의 동일하게 달성함을 보여준다.
  
본 논문의 나머지 부분은 다음과 같이 구성된다. 먼저 2장에서 GBDT 알고리즘과 관련 작업을 검토한다. 3장에서는 GOSS 세부 사항을, 4장에서는 EFB를 소개한다. 공용 데이터셋에 대한 LightGBM 실험은 5장에서 설명한다. 마지막으로 6장에서 결론을 맺는다.  

## 2. 서두
### 2.1 GBDT 및 복잡도 분석

GBDT는 결정 트리를 순차적으로 훈련시키는 앙상블 모형이다. 각 반복마다 GBDT는 음의 기울기(잔차 오류라고도 함)에 적합시켜 의사 결정 트리를 학습한다. GBDT 주요 비용은 의사 결정 트리 학습에서 발생하며 의사 결정 트리를 학습시키는 데 시간 소요가 가장 큰 부분이 최적 분할점 탐색이다. 분할점 탐색을 위해 가장 많이 사용되는 알고리즘 중 하나는 사전 정렬 알고리즘으로 사전 정렬한 변수 값에 대해 가능한 모든 분할점을 나열한다. 이 알고리즘은 간단하며 최적 분할점을 찾아낼 수 있지만 훈련 속도와 메모리 소비 모든 면에서 비효율적이다. 인기있는 다른 알고리즘으로 알고리즘 1[^3]에 적힌 히스토그램 기반 알고리즘이 있다. 정렬한 변수 값에서 분할점을 찾는 대신 히스토그램 기반 알고리즘은 연속적인 변수 값을 개별 구간으로 나누고 이 구간을 사용하여 훈련 시 변수 히스토그램을 만든다. 히스토그램 기반 알고리즘은 메모리 소비와 훈련 속도에서 보다 효율적이므로 이를 바탕으로 작업을 진행했다. 알고리즘 1에 적힌대로 히스토그램 기반 알고리즘은 변수 히스토그램을 기반으로 최적 분할점을 찾는다. 히스토그램 만드는 일에 \\(O(\\# data \times \\# feature)\\), 분할점 찾는 일에 \\(O(\\# bin \times \\# feature)\\)가 필요하다. #bin은 대개 #data보다 훨씬 작기 때문에 히스토그램 만드는 일이 계산 복잡도를 좌우한다. #data 또는 #feature를 줄인다면 GBDT 훈련을 상당히 빠르게 만들 것이다.
- - -
**알고리즘 1:** 히스토그램 기반 알고리즘
- - -
**입력:** \\(I\\): 훈련 데이터, \\(d\\): 최대 깊이, \\(m\\): 변수 개수  
\\(nodeSet \leftarrow \\{0\\} \triangleright\\) 현재 깊이에서의 트리 노드들  
\\(rowSet \leftarrow \\{ \\{ 0, 1, 2, \ldots \\} \\} \triangleright\\) 트리 노드에서의 데이터 색인들
  
**for** i = 1 **to** \\(d\\) **do**
> **for** node **in** \\(nodeSet\\) **do**
>> usedRows \\(\leftarrow rowSet\\)\[node\]  
>> **for** k = 1 **to** \\(m\\) **do**
>>> \\(H \leftarrow\\) new Histogram() \\(\triangleright\\) 히스토그램 생성  
>>> **for** j **in** usedRows **do**
>>>> bin \\(\leftarrow I\\).f\[k\]\[j\].bin  
>>>> \\(H\\)\[bin\].y \\(\leftarrow H\\)\[bin\].y + \\(I\\).y\[j\]  
>>>> \\(H\\)\[bin\].n \\(\leftarrow H\\)\[bin\].n + 1
  
>>> 히스토그램 \\(H\\)의 최적 분할점을 찾는다.  
>>> \\(\ldots\\)
  
>> 최적 분할점을 적용해 \\(rowSet\\)과 \\(nodeSet\\)을 업데이트한다.  
>> \\(\ldots\\) 
  
### 2.2 연관 작업
  
XGBoost, pGBRT, scikit-learn, R의 gbm[^4]을 포함해 GBDT 구현에는 꽤 여러 종류가 있다. Scikit-learn과 R의 gbm은 사전 정렬 알고리즘을 구현했고 pGBRT는 히스토그램 기반 알고리즘을 구현했다. XGBoost는 사전 정렬 알고리즘과 히스토그램 기반 알고리즘 모두 지원한다. 논문[^5]에서 볼 수 있듯이 XGBoost가 다른 것들을 능가한다. 그래서 본 논문 실험 시 XGBoost를 기준선으로 삼았다.
  
훈련 데이터 크기를 줄이기 위한 일반적인 방법은 데이터 개체를 다운 샘플링하는 것이다. 예를 들어 논문[^6]은 데이터 개체 가중치가 고정 임계값보다 작을 경우 해당 데이터 개체는 거른다. SGB는 각 반복마다 약한 학습기를 훈련시키기 위해 임의의 부분 집합을 사용한다. 논문[^7]은 표본 추출 비율을 훈련 과정마다 동적으로 조정한다. 단 SGB를 제외한 모든 작업은 AdaBoost를 기반으로 하는데 GBDT는 Adaboost와 달리 데이터 개체마다 기본 가중치가 없기 때문에 직접 적용하긴 어렵다. SGB는 GBDT에 적용 할수 있지만 일반적으로 정확도가 떨어지므로 바람직한 선택이 아니다.
  
비슷하게 변수 개수를 줄이기 위해 설명력이 약한 변수부터 거르는게 자연스럽다. 이를 위해 주성분 분석이나 사영 추적을 보통 사용한다. 그러나 이런 접근은 변수들이 상당한 중복 요소를 갖고 있다는, 실제로 그렇지 않을 수 있는 가정에 크게 의존한다(일반적으로 변수마다 고유한 기여도를 갖게끔 설계하며 그 중 하나라도 제거하면 훈련 정확도에 어느 정도 영향을 미친다).
  
실제 응용 프로그램에서 사용하는 대규모 데이터셋은 보통 굉장히 희소하다. 사전 정렬 알고리즘을 사용하는 GBDT의 경우 변수값이 0인 개체는 무시함으로써 학습 비용을 줄일 수 있다. 그러나 히스토그램 기반 알고리즘을 사용하는 GBDT의 경우 효율적인 희소 변수 최적화 해법은 없다. 히스토그램 기반 알고리즘은 변수 값 0 여부와 상관없이 각 데이터 개체마다 변수 구간 값(알고리즘 1 참조)을 검색해야하기 때문이다. 히스토그램 기반 알고리즘 적용 GBDT가 희소 변수를 효과적으로 활용할 수 있는 방안이 매우 요구된다.

이전 작업의 한계점을 해결하기 위해 기울기 기반 단측 표본 추출(GOSS)과 배타적 변수 묶음(EFB)이라는 새로운 두 가지 기법을 제시한다. 자세한 내용은 이어지는 절에서 소개할 것이다.
 
## 3. 기울기 기반 단측 표본 추출
  
이번 장에서는 데이터 개체 수를 줄이는 것과 학습한 의사 결정 트리 정확도를 유지하는 것 사이에서 균형을 잘 잡을 수 있는 GBDT의 새로운 표본 추출 방법을 제안한다.

### 3.1 알고리즘 서술
  
AdaBoost에서의 표본 가중치는 각 데이터 개체의 중요도를 알려주는 좋은 지표이다. 그러나 GBDT의 표본은 기본 가중치가 없으므로 AdaBoost에 제안된 표본 추출 방법을 직접 적용할 순 없다. 다행히도 GBDT에서 각 데이터 개체에 대한 기울기는 데이터 표본 추출에 유용한 정보를 제공한다. 즉, 개체의 기울기가 작다면 해당 개체에 대한 훈련 오류가 작고 이미 잘 훈련된 것이다. 단순한 아이디어는 기울기가 작은 데이터 개체를 버리는 것이다. 그러나 그렇게 하면 데이터 분포가 바뀌므로 학습 모형의 정확도를 저하시킬 수 있다. 이 문제를 피하기 위해 기울기 기반 단측 표본 추출(GOSS)이라는 새로운 방법을 제안한다.
  
GOSS는 기울기가 큰 개체는 모두 유지하되 기울기가 작은 개체에 대해 무작위 표본 추출을 수행한다. 이 때 데이터 분포에 미치는 영향을 보상하기 위해서 정보 획득 계산 시 기울기가 작은 데이터 개체에 상수 승수를 적용한다(알고리즘 2 참조). 구체적으로 우선 GOSS는 기울기 절대값에 따라 데이터 개체를 정렬하고 상위 \\(a \times 100 \%\\) 개체를 선택한다. 그런 다음 나머지 데이터에서 \\(b \times 100 \%\\) 개체를 무작위 표본 추출한다. 그 후 GOSS는 정보 획득을 계산할 때 기울기가 작은 표본 데이터를 상수 \\(1 - a \over b\\)만큼 증폭시킨다. 이렇게 함으로써 원래의 데이터 분포를 많이 변경하지 않고도 훈련이 덜 된 개체에 초점을 보다 잘 맞출 수 있다.
- - -
**알고리즘 2:** 기울기 기반 단측 표본 추출
- - -
**입력:** \\(I\\): 훈련 데이터, \\(d\\): 최대 깊이, \\(a\\): 기울기 큰 데이터의 표본 추출 비율  
**입력:** \\(b\\): 기울기 작은 데이터의 표본 추출 비율, \\(loss\\): 손실 함수, \\(L\\): 약한 학습기  
models \\(\leftarrow \\{ \\} \\), fact \\(\leftarrow {1 - a \over b}\\), topN \\(\leftarrow a \times\\) len(\\(I\\)), randN \\(\leftarrow b \times\\) len(\\(I\\))
  
**for** i = 1 **to** \\(d\\) **do**
> preds \\(\leftarrow\\) models.predict(\\(I\\))  
> g \\(\leftarrow loss\\)(\\(I\\), preds), w \\(\leftarrow \\{1, 1, \ldots \\} \\)  
> sorted \\(\leftarrow\\) GetSortedIndices(abs(g))  
> topSet \\(\leftarrow\\) sorted\[1:topN\]  
> randSet \\(\leftarrow\\) RandomPick(sorted\[topN:len(\\(I\\))\], randN)  
> usedSet \\(\leftarrow\\) topSet \\(+\\) randSet  
> w\[randSet\] \\(\times =\\) fact \\(\triangleright\\) 기울기 작은 데이터에 가중치 fact를 부여함  
> newModel \\(\leftarrow\\) \\(L\\)(\\(I\\)\[usedSet\], \\(-\\) g\[usedSet\], w\[usedSet\])  
> models.append(newModel)
  
### 3.2 이론적 분석    
GBDT는 결정 트리를 사용하여 입력 공간 \\(\mathcal{X}^s\\)에서 기울기 공간 \\(\mathcal{G}\\)까지의 함수를 학습한다. \\(n\\)개의 i.i.d 개체 \\(\\{x_1, \ldots, x_n\\}\\)의 훈련 세트를 가지고 있다고 가정하자. 각 \\(x_i\\)는 공간 \\(\mathcal{X}^s\\)내 차원이 \\(s\\)인 벡터다. 그래디언트 부스팅 매 회 반복마다 발생하는, 모형 출력값에 대한 손실 함수의 음의 기울기를 \\(\\{g_1, \ldots, g_n\\}\\)로 정의하자. 의사 결정 트리 모형은 가장 정보력 있는 변수(정보 획득이 가장 큰)로 각 노드를 분할한다. GBDT의 경우 정보 획득은 분할 후 분산으로 보통 측정하며 다음과 같이 정의된다.
  
**정의 3.1** *\\(O\\)를 의사 결정 트리의 미리 정한 노드 안에 있는 훈련 데이터셋라고 하자. 이 노드에 대해 점 \\(d\\)에서 분할하는 변수 \\(j\\)의 분산 획득은 다음과 같이 정의된다.*
  
$$V_{j|O}(d) = {1 \over n_O} \left( {(\sum_{x_i \in O: x_{ij} \le d} g_i )^2 \over n^j_{l|O}(d)} + { ( \sum_{x_i \in O: x_{ij} > d} g_i )^2 \over n^j_{r|O}(d) } \right),$$
  
*여기서 \\(n_O = \sum I\[ x_i \in O \], n^j_{l\|O}(d) = \sum I \[ x_i \in O: x_{ij} \le d \], (n^j_{r\|O}(d) = \sum I \[x_i \in O: x_{ij} > d \]\\)이다.*
  
변수 j에 대해 결정 트리 알고리즘은 \\(d^\*\_j = argmax_d V_j(d)\\)를 선택하고 최대 획득 \\(V_j(d^\*\_j)\\)를 계산한다.[^8] 그런 다음 데이터를 변수 \\(j^\*\\)의 점 \\(d_{j^\*}\\)에 따라 왼쪽과 오른쪽 하위 노드로 분할한다.
  
제안된 GOSS 방법론은 우선 기울기 절대값에 따라 훈련 개체 순위를 내림차순으로 매긴다. 그런 다음 기울기가 큰 상위 \\(\alpha \times 100\%\\) 개체를 가지고 개체 부분 집합 \\(A\\)를 만든다. \\((1 - \alpha) \times 100\%\\)의 기울기가 작은 개체가 속하는 여집합 \\(A^c\\)에 대해 \\(b \times \| A^c \|\\) 크기의 부분 집합 \\(B\\)를 무작위 표본 추출하여 만든다. 마지막으로 부분 집합 \\(A \cup B\\)에 대해 추정 분산 획득 \\(\tilde{V}_j(d)\\)을 적용하여 개체를 분할한다. 추정 분산 획득은
  
$$\tilde{V}_j(d) = {1 \over n} \left( {( \sum_{x_i \in A_l} g_i + {1 - a \over b} \sum_{x_i \in B_l} g_i )^2 \over n^j_l(d)} + {( \sum_{x_i \in A_r} g_i + {1 - a \over b} \sum_{x_i \in B_r} g_i )^2 \over n^j_r(d)} \right),$$
  
이다. 여기서 \\(A_l = \\{x_i \in A: x_{ij} \le d \\}, A_r = \\{x_i \in A: x_{ij} > d \\}, B_l = \\{x_i \in B: x_{ij} \le d \\}, B_r = \\{x_i \in B: x_{ij} > d \\}\\)이고 계수 \\({1 - a \over b}\\)는 \\(B\\)의 기울기 합을 \\(A^c\\) 크기에 맞게 정규화하는데 사용한다.

따라서 GOSS는 분할점 결정 시 모든 개체를 사용한 정확한 \\(\tilde{V}_j(d)\\) 대신 더 적은 수의 개체 부분 집합을 통해 추정한 \\(V_j(d)\\)를 이용하므로 계산 비용을 크게 줄일 수 있다. 보다 중요하게, 다음 정리는 GOSS가 훈련 정확도를 많이 잃지 않고 무작위 표본 추출보다 우수함을 보여준다. 지면의 제약으로 보충 자료에 정리의 증명을 남긴다.

**정리 3.2** *GOSS의 근사 오차를 \\(\mathcal{E}(d) = \| \tilde{V}\_j(d) - V_j(d) \|\\)로 정의하고 \\(\bar{g}^j_l(d) = { \sum_{x_i \in ( A \cup A^c )_l} \| g_i \| \over n^j_l(d)}, \bar{g}^j_r(d) = { \sum_{x_i \in ( A \cup A^c )_r} \| g_i \| \over n^j_r(d)}\\)라고 하자. 적어도 \\(1 - \delta\\)의 확률로*
  
$$\mathcal{E}(d) \le C^2_{a, b} \ln 1/\delta \cdot max \left\{ {1 \over n^j_l(d)}, {1 \over n^j_r(d)} \right\} + 2DC_{a, b}\sqrt{\ln 1 / \delta \over n},$$
  
*이다. 여기서 \\(C_{a, b} = {1 - a \over \sqrt{b}} max_{x_i \in A^c} \|g_i\|\\)이고 \\(D = max(\bar{g}^j_l(d), \bar{g}^j_r(d))\\)이다.*

정리에 의거해 다음과 같이 논의해볼 수 있다. (1) GOSS의 점근적 근사 비율은 \\(\mathcal{O} \left( {1 \over n^j_l(d)} + {1 \over n^j_r(d)} + {1 \over \sqrt{n}} \right)\\)이다. 한쪽으로 치우쳐 분할되지 않았다면(즉, \\(n^j_l(d) \ge \mathcal{O}(\sqrt{n})\\)이거나 \\(n^j_r(d) \ge \mathcal{O}(\sqrt{n})\\)), \\(n \rightarrow \infty\\)일 때 \\(\mathcal{O}(\sqrt{n})\\) 속도로 0으로 감소하는 부등식 두 번째 항이 근사 오차를 좌우한다. 즉, 데이터 수가 많다면 근사값은 상당히 정확하다. (2) 무작위 표본 추출은 \\(a = 0\\)인 GOSS의 특수한 경우다. 많은 경우 GOSS는 \\({\alpha_a \over \sqrt{\beta}} > {1 - \alpha_a \over \sqrt{\beta - \alpha}}\\) (여기서 \\(\alpha_a = max_{x_i \in A \cup A^c} \|g_i\| / max_{x_i \in A^c} \|g_i\\))와 동일한 \\(C_{0, \beta} >  C_{\alpha, \beta - \alpha}\\) 조건 하에서 무작위 표본 추출보다 우수하다. 
  
다음으로 GOSS의 일반화 성능을 분석했다. GOSS의 일반화 오차 \\(\mathcal{E}^{GOSS}\_{gen}(d) = \| \tilde{V}\_j(d) - V_\*(d) \|\\)는 GOSS에서 표본 추출 된 훈련 인스턴스에 의해 계산 된 분산 이득과 기본 배포. 우리는 \\(\mathcal{E}^{GOSS}\_{gen}(d) \le \| \tilde{V}_j(d) - V_j(d) \| + \| V_j(d) - V_\*(d) \| \triangleq \mathcal{E}_{GOSS}(d) + \mathcal{E}_{gen}(d)\\). 따라서 GOSS 근사화가 정확하다면 GOSS의 일반화 오류는 전체 데이터 인스턴스를 사용하여 계산 된 오류와 비슷할 것입니다. 반면에 표본 추출은 기본 학습자의 다양성을 증가시켜 잠재적으로 일반화 성능을 향상시키는 데 도움을 줄 것입니다.

## 4. 배타적 변수 묶음
    
(번역 중)

[^1]: 본 논문에서 기울기 크고 작음의 기준은 절대값이다.
[^2]: 코드는 Github에서 구할 수 있다. [https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM)
[^3]: 지면의 제약으로 인해 높은 수준의 의사 코드를 사용했다. 자세한 내용은 오픈 소스 코드에서 찾을 수 있다.
[^4]: GPU 또는 병렬 훈련을 통해 GBDT 훈련을 가속화시키는 몇 가지 다른 작업물이 있다. 그러나 이 논문의 범위를 벗어난다.
[^5]: Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785–794. ACM, 2016.
[^6]: Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):337–407, 2000.
[^7]: Charles Dubout and François Fleuret. Boosting with maximum adaptive sampling. In Advances in Neural Information Processing Systems, pages 1332–1340, 2011.
[^8]: 다음 분석은 임의의 노드에 적용된다. 단순화하고 혼동되지 않기 위해 모든 표기법에서 하위 색인 \\(O\\)를 생략했다.
