---
layout: post
title: LightGBM 고효율 그래디언트 부스팅 결정 트리
date: 2018-06-29 00:00:00
author: Microsoft Research, Peking University, Microsoft Redmond
categories: Data-Science
---  
  
  
**Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu의 [*LightGBM: A Highly Efficient Gradient Boosting Decision Tree*](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)을 번역했습니다.**
  
  
- - -
  
## 초록
그래디언트 부스팅 결정 트리(GBDT)는 널리 사용되는 기계 학습 알고리즘이며 XGBoost와 pGBRT 같이 효율적으로 구현해놓은 것이 몇 가지 있다. 해당 구현은 엔지니어링의 많은 요소를 최적화시켰지만 고차원 변수에 데이터 크기가 큰 경우 효율성과 확장성은 여전히 불만족스럽다. 주된 이유로 각 변수마다 가능한 모든 분할점에 대해 정보 획득을 평가하려면 데이터 개체 모두 훑어야하는데 이에 많은 시간이 소요된다는 점이다. 이 문제를 해결하기 위해 *기울기 기반 단측 표본추출*(GOSS)과 *배타적 변수 묶음*(EFB)이라는 새로운 기술 두 가지를 제안한다. GOSS를 통해 데이터 개체 중 기울기가 작은 상당 부분을 제외시키고 나머지만 사용하여 정보를 얻을 수 있다. 기울기가 큰 데이터 개체가 정보 획득 계산에 더 중요한 역할을 하기에 GOSS는 훨씬 작은 크기의 데이터로 정보 획득을 매우 정확하게 추정할 수 있다. EFB를 통해 변수 개수를 줄이기 위해 상호 배타적 변수들(예컨대, 0이 아닌 값을 동시에 갖는 일이 거의 없는 변수들)을 묶는다. 배타적 변수의 최적 묶음을 찾는 일은 NP-hard지만 탐욕 알고리즘을 통해 매우 좋은 근사 비율을 얻을 수 있다. 따라서 분할점 결정 정확도를 크게 훼손시키지 않으면서 변수 개수를 효과적으로 줄일 수 있다. GOSS와 EFB 적용하여 GBDT를 새롭게 구현한 걸 *LightGBM*라고 부르겠다. 여러 공용 데이터셋에 대한 실험은 LightGBM이 기존 GBDT 훈련 과정을 최대 20배 이상 빠르게 하면서 정확도는 거의 동일하게 달성함을 보여준다.
  
## 1. 개론
그래디언트 부스팅 결정 트리(GBDT)는 효율성, 정확도, 해석 가능성이 높아 널리 사용되는 기계 학습 알고리즘이다. GBDT는 멀티 클래스 분류, 클릭 예측, 순위 학습 같이 다양한 기계 학습 작업에서 최고의 성능을 보여준다. 최근 빅데이터(변수 개수와 개체 수 모두) 등장으로 GBDT는 특히 정확도와 효율성 간의 트레이드오프라는 새로운 숙제에 직면했다. GBDT 기존 구현은 각 변수마다 가능한 모든 분할점에 대해 정보 획득을 평가하기 위해 데이터 개체 모두 훑어야한다. 따라서 계산 복잡도는 변수 개수와 개체 수에 비례한다. 따라서 빅데이터 처리 시 해당 구현은 매우 시간 소모적이게 된다.
  
데이터 개체 수와 변수 개수를 줄이는게 문제 해결을 위한 즉각적 아이디어다. 그러나 이 작업은 꽤나 단순하지 않다. 예를 들어 GBDT를 위해 데이터 표본 추출 수행하는 방법이 불명확하다. 부스팅 훈련 과정의 속도를 향상시키기 위해 가중치에 따라 데이터 표본 추출하는 작업물이 있지만 GBDT에는 표본 가중치가 전혀 없으므로 직접 적용할 수 없다. 본 논문은 목표를 위해 두 가지 새로운 기술을 제안한다.
  
Gradient-based One-Side Sampling (GOSS). GBDT의 데이터 인스턴스에 대한 기본 가중치는 없지만, 서로 다른 그래디언트를 가진 데이터 인스턴스가 정보 획득 계산에서 다른 역할을한다는 것을 알 수 있습니다. 특히, 정보 획득의 정의에 따르면,보다 큰 그라디언트 (즉, 과소 훈련 된 인스턴스)를 갖는 인스턴스는 정보 획득에 더 기여할 것이다. 따라서 데이터 인스턴스를 다운 샘플링 할 때 정보 이득 추정의 정확성을 유지하려면 이러한 인스턴스를 큰 그라디언트 (예 : 미리 정의 된 임계 값보다 크거나 맨 위 백분위 수)보다 많이 유지해야하며 무작위로만 유지해야합니다 그라데이션이 작은 인스턴스를 드롭합니다. 우리는 그러한 처리가 동일한 표적 샘플링 속도로 균일 무작위 표본 추출보다 더 정확한 이득 추정으로 이어질 수 있음을 증명합니다. 특히 정보 이득의 값이 큰 범위를 갖는 경우 특히 그렇습니다.
  
Exclusive Feature Bundling (EFB). 일반적으로 실제 응용 프로그램에서는 많은 수의 기능이 있지만 기능 공간이 매우 희박하여 효과적인 기능의 수를 줄이기 위해 거의 무손실 방식을 설계 할 수 있습니다. 특히, 희소 한 특징 공간에서, 많은 특징들이 (거의) 배타적이다. 즉, 그들은 0이 아닌 값을 거의 취하지 않는다. 예를 들어, 원 핫 기능 (예 : 텍스트 마이닝의 원 핫 단어 표현)이 있습니다. 이러한 독점적 인 기능을 안전하게 묶을 수 있습니다. 이를 위해 최적의 번들링 문제를 그래프 착색 문제 (vertex로 피쳐를 취하고 두 피쳐가 상호 배타적이지 않으면 두 피처 모두에 에지를 추가 함으로서)를 줄임으로써 효율적인 알고리즘을 설계하고, 다음과 같은 greedy 알고리즘으로 문제를 해결합니다. 일정 근사 비율.
  
우리는 GOSS와 EFB LightGBM2를 사용하여 새로운 GBDT 알고리즘을 호출합니다. 여러 개의 공용 데이터 세트에 대한 실험을 통해 LightGBM은 거의 동일한 정확도를 달성하면서 최대 20 회까지 교육 프로세스를 가속화 할 수 있음을 보여줍니다.
  
이 논문의 나머지 부분은 다음과 같이 구성됩니다. 처음에는 GBDT 알고리즘과 관련 연구를 Sec. 2. 그런 다음 GOSS의 세부 사항을 초에 소개합니다. 3 및 EFB in Sec. 4. 공개 데이터 세트에 대한 LightGBM에 대한 실험은 Sec. 5. 마지막으로 우리는 Sec. 6.
  
(번역 중)
