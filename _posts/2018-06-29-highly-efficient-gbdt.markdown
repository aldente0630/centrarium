---
layout: post
title: LightGBM 고효율 그래디언트 부스팅 결정 트리
date: 2018-06-29 00:00:00
author: Microsoft Research, Peking University, Microsoft Redmond
categories: Data-Science
---  
  
  
**Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu의 [*LightGBM: A Highly Efficient Gradient Boosting Decision Tree*](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)을 번역했습니다.**
  
  
- - -
  
## 초록
그래디언트 부스팅 결정 트리(GBDT)는 널리 사용되는 기계 학습 알고리즘이며 XGBoost와 pGBRT 같이 효율적으로 구현해놓은 것이 몇 가지 있다. 해당 구현은 엔지니어링의 많은 요소를 최적화시켰지만 고차원 변수에 데이터 크기가 큰 경우 효율성과 확장성은 여전히 불만족스럽다. 주된 이유로 각 변수마다 가능한 모든 분할점에 대해 정보 획득을 평가하려면 데이터 개체 모두 훑어야하는데 이에 많은 시간이 소요된다는 점이다. 이 문제를 해결하기 위해 *기울기 기반 단측 표본추출*(GOSS)과 *배타적 변수 묶음*(EFB)이라는 새로운 기술 두 가지를 제안한다. GOSS를 통해 데이터 개체 중 기울기가 작은 상당 부분을 제외시키고 나머지만 사용하여 정보를 얻을 수 있다. 기울기가 큰 데이터 개체가 정보 획득 계산에 더 중요한 역할을 하기에 GOSS는 훨씬 작은 크기의 데이터로 정보 획득을 매우 정확하게 추정할 수 있다. EFB를 통해 변수 개수를 줄이기 위해 상호 배타적 변수들(예컨대, 0이 아닌 값을 동시에 갖는 일이 거의 없는 변수들)을 묶는다. 배타적 변수의 최적 묶음을 찾는 일은 NP-hard지만 탐욕 알고리즘을 통해 매우 좋은 근사 비율을 얻을 수 있다. 따라서 분할점 결정 정확도를 크게 훼손시키지 않으면서 변수 개수를 효과적으로 줄일 수 있다. GOSS와 EFB 적용하여 GBDT를 새롭게 구현한 걸 *LightGBM*라고 부르겠다. 여러 공용 데이터셋에 대한 실험은 LightGBM이 기존 GBDT 훈련 과정을 최대 20배 이상 빠르게 하면서 정확도는 거의 동일하게 달성함을 보여준다.
  
## 1. 개론
그래디언트 부스팅 결정 트리(GBDT)는 효율성, 정확도, 해석 가능성이 높아 널리 사용되는 기계 학습 알고리즘이다. GBDT는 멀티 클래스 분류, 클릭 예측, 순위 학습 같이 다양한 기계 학습 작업에서 최고의 성능을 보여준다. 최근 빅데이터(변수 개수와 개체 수 모두) 등장으로 GBDT는 특히 정확도와 효율성 간의 트레이드오프라는 새로운 숙제에 직면했다. GBDT 기존 구현은 각 변수마다 가능한 모든 분할점에 대해 정보 획득을 평가하기 위해 데이터 개체 모두 훑어야한다. 따라서 계산 복잡도는 변수 개수와 개체 수에 비례한다. 따라서 빅데이터 처리 시 해당 구현은 매우 시간 소모적이게 된다.
  
데이터 개체 수와 변수 개수를 줄이는게 문제 해결을 위한 즉각적 아이디어다. 그러나 이 작업은 꽤나 단순하지 않다. 예를 들어 GBDT를 위해 데이터 표본 추출 수행하는 방법이 불명확하다. 부스팅 훈련 과정의 속도를 향상시키기 위해 가중치에 따라 데이터 표본 추출하는 작업물이 있지만 GBDT에는 표본 가중치가 전혀 없으므로 직접 적용할 수 없다. 본 논문은 목표를 위해 두 가지 새로운 기술을 제안한다.
  
*기울기 기반 단측 표본추출*(GOSS). GBDT의 경우 데이터 개체에 대한 기본 가중치는 없지만 서로 다른 기울기를 가진 데이터 개체가 정보 획득 계산 시 서로 다른 역할을 한다는 점은 알고있다. 정보 획득의 정의로 보자면 기울기가 보다 큰(즉, 과소 훈련시킨 개체)개체가 정보 획득에 더 기여할 것이다. 따라서 데이터 개체를 다운샘플링할 때 정보 이득 추정의 정확도를 유지하려면 기울기가 큰 개체(예컨대 미리 정의한 임계 값보다 크거나 백분위로 상위인)를 보다 많이 유지해야하며 기울기가 작은 개체는 무작위로 떨궈야한다. 이런 방식이 타겟 변수마다 동일한 표본 추출 비율로 균일하게 무작위 표본 추출하는 것보다 더 정확하게 정보 획득을 추정할 수 있음(정보 획득 값의 범위가 큰 경우 특히나)을 증명해낼 것이다. 
  
*배타적 변수 묶음*(EFB). 실제 응용 프로그램은 일반적으로 많은 수의 변수를 갖지만 변수 공간은 매우 희소하여 변수 개수를 효과적으로 줄이기 위한 거의 손실 없는 방식을 만들어낼 수 있다. 특히 희소한 변수 공간에서 많은 변수들이 (거의) 배타적이다. 즉, 변수들은 0이 아닌 값을 동시에 갖는 일이 거의 없다 . 예로 원-핫 변수(텍스트 마이닝의 원-핫 단어 표현 같은)가 있다. 이런 배타적 변수들은 안전하게 묶을 수 있다. 결국 최적 묶음 문제를 그래프 채색 문제(변수를 각 점에 두고 두 변수가 상호 배타적이지 않으면 두 변수를 잇는 선을 추가함으로써)로 바꾸는 효율적인 알고리즘을 설계했고 일정 근사 비율을 갖는 탐욕 알고리즘으로 문제를 해결했다.
  
GOSS와 EFB 적용하여 GBDT를 새롭게 구현한 걸 *LightGBM*라고 부르겠다. 여러 공용 데이터셋에 대한 실험은 LightGBM이 기존 GBDT 훈련 과정을 최대 20배 이상 빠르게 하면서 정확도는 거의 동일하게 달성함을 보여준다.
  
본 논문의 나머지 부분은 다음과 같이 구성된다. 먼저 2장에서 GBDT 알고리즘과 관련 작업을 검토한다. 3장에서는 GOSS 세부 사항을, 4장에서는 EFB를 소개한다. 공용 데이터셋에 대한 LightGBM 실험은 5장에서 설명한다. 마지막으로 6장에서 결론을 맺는다.  

## 2. 선행요건
### 2.1 GBDT 및 복잡도 분석

GBDT는 순차적으로 훈련시킨 결정 트리 앙상블 모형이다. 각 반복마다 GBDT는 음의 기울기(잔차 오류라고도 함)에 적합시켜 의사 결정 트리를 학습한다. GBDT의 주요 비용은 의사 결정 나무를 학습하는 데 있으며 의사 결정 트리를 학습하는 데 가장 시간이 많이 소요되는 부분은 최상의 분할 지점을 찾는 것입니다. 분할 점을 찾는 데 가장 많이 사용되는 알고리즘 중 하나는 사전 정렬 된 알고리즘으로 사전 정렬 된 특징 값에 가능한 모든 분할 점을 나열합니다. 이 알고리즘은 간단하고 최적의 분리 점을 찾을 수 있지만 교육 속도와 메모리 소비 모두에서 비효율적입니다. 또 다른 인기있는 알고리즘은 Alg에서 볼 수 있듯이 히스토그램 기반 알고리즘입니다. 13. 히스토그램 기반 알고리즘 버킷은 정렬 된 특징 값에서 분할 점을 찾는 대신 특성 값을 개별 상자로 나누고 트레이를 사용하여이 막대를 사용하여 특징 히스토그램을 만듭니다. 히스토그램 기반 알고리즘은 메모리 소비와 교육 속도면에서 더 효율적이므로, 우리는이를 바탕으로 작업을 개발할 것입니다. Alg. 1에서 히스토그램 기반 알고리즘은 특징 히스토그램을 기반으로 최상의 분할 점을 찾습니다. 히스토그램 작성을 위해서는 \\(O(\#data \times \#feature)\\), 분리 점을 찾는 데는 \\(O(#bin \times #feature)\\)가 필요합니다. #bin은 대개 #data보다 훨씬 작기 때문에 히스토그램 작성이 계산상의 복잡성을 압도합니다. # 데이터 또는 # 피쳐를 줄일 수 있다면 우리는 GBDT의 교육을 상당히 가속화 할 수 있습니다.

### 2.2 연관 작업

(번역 중)
