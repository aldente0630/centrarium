---
layout: post
title: LightGBM 고효율 그래디언트 부스팅 결정 트리
date: 2018-06-29 00:00:00
author: Microsoft Research, Peking University, Microsoft Redmond
categories: Data-Science
---  
  
  
**Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu의 [*LightGBM: A Highly Efficient Gradient Boosting Decision Tree*](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)을 번역했습니다.**
  
  
- - -
  
## 초록
그래디언트 부스팅 결정 트리(GBDT)는 널리 사용되는 기계 학습 알고리즘이며 XGBoost와 pGBRT 같이 효율적으로 구현해놓은 것이 몇 가지 있다. 해당 구현은 엔지니어링의 많은 요소를 최적화시켰지만 고차원 변수에 데이터 크기가 큰 경우 효율성과 확장성은 여전히 불만족스럽다. 주된 이유로 각 변수마다 가능한 모든 분할점에 대해 정보 획득을 평가하려면 데이터 개체 모두 스캔해야하는데 이에 많은 시간이 소요된다는 점이다. 이 문제를 해결하기 위해 *기울기 기반 단측 표본추출*(GOSS)과 *배타적 변수 묶음*(EFB)이라는 새로운 기술 두 가지를 제안한다. GOSS를 통해 데이터 개체 중 기울기가 작은 상당 부분을 제외시키고 나머지만 사용하여 정보를 얻을 수 있다. 기울기가 큰 데이터 개체가 정보 획득 계산에 더 중요한 역할을 하기에 GOSS는 훨씬 작은 크기의 데이터로 정보 획득을 매우 정확하게 추정할 수 있다. EFB를 통해 변수 개수를 줄이기 위해 상호 배타적 변수들(예컨대, 0이 아닌 값을 동시에 갖는 일이 거의 없는 변수들)을 묶는다. 배타적 변수의 최적 묶음을 찾는 일은 NP-hard지만 탐욕 알고리즘을 통해 매우 좋은 근사 비율을 얻을 수 있다. 따라서 분할점 결정 정확도를 크게 훼손시키지 않으면서 변수 개수를 효과적으로 줄일 수 있다. GOSS와 EFB 적용하여 GBDT를 새롭게 구현한 걸 *LightGBM*라고 부르겠다. 여러 공용 데이터셋에 대한 실험은 LightGBM이 기존 GBDT 훈련 과정을 최대 20배 이상 빠르게 하면서 정확도는 거의 동일하게 달성함을 보여준다.
  
## 1. 개론
그래디언트 부스팅 결정 트리(GBDT)는 효율성, 정확도, 해석 가능성이 높아 널리 사용되는 기계 학습 알고리즘이다. GBDT는 멀티 클래스 분류, 클릭 예측, 순위 학습 같이 다양한 기계 학습 작업에서 최고의 성능을 보여준다. 최근에는 빅데이터(변수 개수와 개체 개수 모두)의 등장으로 GBDT는 특히 정확도와 효율성 간의 절충안에서 새로운 숙제에 직면해 있다. GBDT의 기존 구현은 모든 기능에 대해 가능한 모든 분할 지점의 정보 획득을 평가하기 위해 모든 데이터 인스턴스를 스캔해야합니다. 따라서 계산 복잡도는 피처 수와 인스턴스 수에 비례합니다. 따라서 큰 데이터를 처리 할 때 이러한 구현은 매우 시간 소모적입니다.
  
이러한 문제를 해결하기 위해 데이터 인스턴스 수와 기능 수를 줄이는 것이 직접적인 아이디어입니다. 그러나 이것은 매우 사소한 것으로 밝혀졌습니다. 예를 들어, GBDT에 대한 데이터 샘플링을 수행하는 방법이 불분명합니다. 부싱 훈련 과정 속도를 높이기 위해 가중치에 따라 데이터를 샘플링하는 작업이 있지만 [5, 6, 7], GBDT에 샘플 가중치가 전혀 없으므로 GBDT에 직접 적용 할 수 없습니다. 이 논문에서, 우리는이 목표를 향한 두 가지 새로운 기술을 제안한다.
  
(번역 중)
